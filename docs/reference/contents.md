# Project Contents

SeqQuests is designed for protein sequence analysis, focusing on a pipeline for all-on-all Swiss-Prot sequence comparison. It has 
* A high-performance local similarity comparisons (Smith-Waterman), 
* Reduction of the results using a maximum scoring tree, and then 
* Interactive exploratory analysis of those reduced results.

## Script and UI
* The code canbe used both as scripts with a purely textual input and output, and also via web UI interfaces. 
  * Wrappers over text UI give interactive progress indicators, for example tracking 'best so far' from a stream of results.
  * The Web UI cross references sequence IDs to sequence metadata and dynamically presents alignments corresponding to a particular find.

## AI Relevance
* The code has been written with AI assistance. It is designed to factorise into independently analysable components.
* AI has been instrumental in transforming code between different formats, python, C++, metal, Javascript.
* The search console has been used for queries to an LLM, "Please provide more background on these two proteins. Why would these be similar? Is their similarity unexpected?", queries that can be directly pasted into the LLM for comment.

## Python, C++ and Metal
* Most of the algorithms exist in a python version with sometimes C++ direct equivalents where speed is required.
  * LLMs can generate and debug python more rapidly than C++. 
  * In python I can easily call on the vast range of pre built python modules. This allows me to keep the C++ more bare bones.
  * C++ versions, including arg parsing, can be direct translations of core algorithms that need to run fast. These take pre-prepared indexed data as their database input, rather than read from text files.
  * Metal versions exist for the long-running code that needs the greatest speed.

## 1. Core Applications

### Web Dashboard
**Entry Point:** `python py/web_server.py` (Run from project root)
**URL:** `http://localhost:8000` (Default)
**Status:** Mature
**Description:** A FastAPI-based web server that provides an API for protein analysis work.

### Web UI
**Entry Point:** `static/lcars.html`
**Entry Point:** `static/job_management.html`
**Entry Point:** `static/match_explorer.html`
**Description:** These provide front ends to the sequence analysis software.

job_management maintains an active jobs list. It allows you to configure and run Data Munging (conversions of file formats for speed or making subsets), Computation (Tree Building from search results), and Sequence Search jobs (long running all-on-all sequence comparisons). This UI allows multiple simultaneous jobs to be run, paused, resumed. Users can connect to the running jobs display from a remote machine and view the configuration parameters that were used for a job.

match_explorer is for active exploring results of the all-on-all reduced results. It displays finds, showing more information about pairs of proteins, including alignments and dotplots.

lcars is an overall management console which integrates and reuses the panels of job_management and match_explorer. It also provides help information.

### Sequence Search (SW)
**Executable:** `bin/sw_search_metal` (Compiled from `c_src/sw_search_metal.mm`)
**Python Wrapper:** `py/sw_search.py`
**Description:** A Metal-accelerated (GPU) implementation of the Smith-Waterman algorithm. It performs massive parallel searches of a query sequence against a database, producing protein similarity data - a list of high scoring links.
*   **Usage:** Typically invoked via the Web Dashboard. Can be run standalone via CLI.
*   **Data:** Requires binary compiled data (`fasta.bin`, `pam250.bin`) generated by `py/prepare_binary_data.py`.

Typically this program is run iterating over all sequences in a database, to get an all on all comparison of proteins. The program can be interrupted, and when it resumes check its links file to see where to resume from.

### Tree Builder
**Script:** `py/tree_builder.py`
**Backend:** `bin/tree_builder_cpp` (Optional C++ acceleration)
**Description:** Constructs Minimum Spanning Trees (MST) from protein similarity data.
*   **Usage:** Configurable via `py/computation.py` jobs in the dashboard.

### Data Munger
This is dummy code, that was used in test and development.
**Script:** `py/data_munger.py`
**Description:** Filters and processes raw Swiss-Prot data to extract relevant proteins based on criteria (e.g., taxonomy, existence of GO terms).

## 2. Python Modules (`py/`)

*   **`job_manager.py`**: The orchestration engine. Manages the lifecycle (start, stop, status) of background jobs triggered by the web server.
*   **`sequences.py`**: The data access layer. Abstracts reading from Swiss-Prot (`.dat`) and FASTA files. Handles caching (Pickle) and Indexing for performance.
*   **`sw_align.py`**: A pure Python implementation of Smith-Waterman. Useful for debugging and verifying scores against the Metal implementation.
*   **`pam_converter.py`**: Utility to convert Biopython's PAM250 matrix into the specific 32x32 integer format required by the Metal kernel.
*   **`computation.py`**: Defines and manages `ComputationJob`s, primarily for Tree Building tasks.
*   **`taxa_lca.py`**: Utilities for determining the Lowest Common Ancestor (LCA) in taxonomy trees.
*   **`swiss_to_pdb.py`**: Tools for mapping Swiss-Prot entries to Protein Data Bank (PDB) structures.
*   **`kabsch_3d_align.py`**: Implementation of the Kabsch algorithm for aligning 3D protein structures.
*   **`web_server.py`**: The main FastAPI application serving the API and static files.
*   **`command_runner.py`**: Helper to run shell commands.
*   **`prepare_binary_data.py`**: Compiles FASTA and PAM data into binary formats for Metal SW.
*   **`config.py`**: Configuration settings (paths, etc).

## 3. Metal & C++ Source (`c_src/`)

*   **`c_src/sw_search_metal.mm`**: The production C++/Metal implementation of the search algorithm.
*   **`c_src/sw.metal`**: The Metal shader kernel code.
*   **`c_src/tree_builder.cpp`**: C++ implementation of the tree builder logic (MST).
*   **`dev/dev_sw_search_metal.py`**: **Prototype.** A Python implementation that mimics the Metal kernel logic. Useful for debugging the algorithm logic without a GPU.

## 4. Web Interface (`static/`)

Contains the frontend code (HTML/JS) for the dashboard.
*   `lcars.html`: The main LCARS interface container.
*   `job_management.html`: Standalone job management page.
*   `match_explorer.html`: Standalone match explorer page.
*   `lcars.js`, `lcars.css`: Main UI logic and styling for the LCARS interface.
*   `app.js`: Main application logic for job management.
*   `job_monitor.js`: Shared JavaScript logic for configuration and job status monitoring.
*   `monitor.js`: Specific monitoring logic.
*   `stream.js`: Handles streaming output from jobs.
*   `style.css`: Global stylesheets.
*   `doclist.js`: List of documentation files.

## 5. Web Interface Panels (`static/panels`)
Full HTML documents that typically provide panels for use within the dashboards.

## 6. Web Interface (`static/docs`)
User facing documentation

## 7.  Setup & Compilation

*   **Compile Native Code:** Run `./compile.sh` to build `bin/sw_search_metal` and `bin/tree_builder_cpp`.
