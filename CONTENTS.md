# Project Contents and User Guide

This suite of tools is designed for protein sequence analysis, specifically focusing on a pipeline for all-on-all Swiss-Prot sequence comparison. It has a high-performance local similarity comparisons (Smith-Waterman), reduction of the results using a maximum scoring tree, and then interactive exploratory analysis of those reduced results.

## Script and UI
* The code is designed to be used both as scripts with a purely textual input and output, and also via web UI interfaces. Web UI is optional, but it is generally useful when browsing results or monitoring progress of search.
  * Wrappers over text UI can give give interactive progress indicators, for example tracking 'best so far' from a stream of results.
  * The underlying search engine works in terms of sequence numbers. The Web UI cross references sequence IDs to sequence metadata and dynamically present alignments corresponding to a particular find.

## AI Relevance
* The code has been written with some LLM assistance, and is designed to factorise into independently analysable components. 
* LLMs have been instrumental in transforming code between different formats, python, C++, metal, Javascript.
* The search console reformats finds as queries to an LLM, "Please provide more background on these two proteins. Why would these be similar? Is their similarity unexpected?" that can be pasted into the LLM for comment.

## Python, C++ and Metal
* Most of the algorithms exist in a python version with sometimes C++ direct equivalents where speed is required.
  * LLMs can generate and debug python more rapidly than C++. In python I can easily call on the vast range of pre built python modules. I keep the C++ more bare bones.
  * C++ versions, including arg parsing, can be direct translations of core algorithms that need to run fast. These take pre-prepared indexed data as their database input, rather than read from text files.
  * Metal versions exist for the long-running code that needs the greatest speed.

## 1. Core Applications

### Web Dashboard
**Entry Point:** `python py/web_server.py` (Run from project root)
**URL:** `http://localhost:8000` (Default)
**Status:** Mature
**Description:** A FastAPI-based web server that provides an API for protein analysis work. 

### Web UI
**Entry Point:** `static/lcars.html`
**Entry Point:** `static/job_management.html`
**Entry Point:** `static/match_explorer.html`
**Status:** Mature
**Description:** These provide front ends to the sequence analysis software. 

job_management maintains an active jobs list. It allows you to configure and run Data Munging (conversions of file formats for speed or making subsets), Computation (Tree Building from search results), and Sequence Search jobs (long running all-on-all sequence comparisons). This UI allows multiple simultaneous jobs to be run, paused, resumed. Users can connect to the running jobs display from a remote machine and view the configuration parameters that were used for a job.

match_explorer is for active exploring results of the all-on-all reduced results. It displays finds, showing more information about pairs of proteins, including alignments and dotplots.

lcars is an overall management console which integrates and reuses the panels of job_management and match_explorer. It also provides help information.

### Sequence Search (SW)
**Executable:** `bin/sw_search_metal` (Compiled from `c_src/sw_search_metal.mm`)
**Python Wrapper:** `py/sw_search.py`
**Status:** Mature (High Performance)
**Description:** A Metal-accelerated (GPU) implementation of the Smith-Waterman algorithm. It performs massive parallel searches of a query sequence against a database, producing protein similarity data - a list of high scoring links.
*   **Usage:** Typically invoked via the Web Dashboard. Can be run standalone via CLI.
*   **Data:** Requires binary compiled data (`fasta.bin`, `pam250.bin`) generated by `c_src/prepare_data.py`.

Typically this program is run iterating over all sequences in a database, to get an all on all comparison of proteins. The program can be interrupted, and when it resumes check its links file to see where to resume from.

### Tree Builder
**Script:** `py/tree_builder.py`
**Backend:** `bin/tree_builder_cpp` (Optional C++ acceleration)
**Status:** Active
**Description:** Constructs Minimum Spanning Trees (MST) from protein similarity data.
*   **Usage:** Configurable via `py/computation.py` jobs in the dashboard.

### Data Munger
**Script:** `py/data_munger.py`
**Status:** Active
**Description:** Filters and processes raw Swiss-Prot data to extract relevant proteins based on criteria (e.g., taxonomy, existence of GO terms).

## 2. Python Modules (`py/`)

*   **`job_manager.py`**: The orchestration engine. Manages the lifecycle (start, stop, status) of background jobs triggered by the web server.
*   **`sequences.py`**: The data access layer. Abstracts reading from Swiss-Prot (`.dat`) and FASTA files. Handles caching (Pickle) and Indexing for performance.
*   **`sw_align.py`**: A pure Python implementation of Smith-Waterman. Useful for debugging and verifying scores against the Metal implementation.
*   **`pam_converter.py`**: Utility to convert Biopython's PAM250 matrix into the specific 32x32 integer format required by the Metal kernel.
*   **`computation.py`**: Defines and manages `ComputationJob`s, primarily for Tree Building tasks.
*   **`taxa_lca.py`**: Utilities for determining the Lowest Common Ancestor (LCA) in taxonomy trees.
*   **`swiss_to_pdb.py`**: Tools for mapping Swiss-Prot entries to Protein Data Bank (PDB) structures.
*   **`kabsch_3d_align.py`**: Implementation of the Kabsch algorithm for aligning 3D protein structures.

## 3. Metal & C++ Source (`c_src/`, `metal/`)

*   **`c_src/sw_search_metal.mm`**: The production C++/Metal implementation of the search algorithm.
*   **`c_src/sw.metal`**: The Metal shader kernel code.
*   **`metal/sw_search_metal.py`**: **Prototype.** A Python implementation that mimics the Metal kernel logic. Useful for debugging the algorithm logic without a GPU.

## 4. Web Interface (`static/`)

Contains the frontend code (HTML/JS) for the dashboard.
*   `index.html`: Main layout.
*   `jobs.html`: Job list and creation.
*   `config_*.html`: Configuration forms for specific job types.
*   `app.js`: Main application logic, handling global polling and UI updates.
*   `job_monitor.js`: Shared JavaScript logic for configuration iframes and job status monitoring.
*   `monitor.js`: Specific monitoring logic.
*   `style.css`: Global stylesheets.
*   `monitor.css`: Styles specific to the job monitor interface.
*   `stream.js`: Handles streaming output from jobs.

## 5. Web Interface Panels (`static/partials`)
Chunks of HTML that typically provide panels for use within the dashboards.

## 6. Web Interface (`static/docs`)
User facing documentation

## 7.  Setup & Compilation

*   **Compile Native Code:** Run `./compile.sh` to build `bin/sw_search_metal` and `bin/tree_builder_cpp`.
*   **Python Path:** Ensure `PYTHONPATH` includes the `py/` directory (e.g., `PYTHONPATH=py python py/web_server.py`).
