# Project Contents and User Guide

This suite of tools is designed for protein sequence analysis, specifically focusing on high-performance local similarity comparisons (Smith-Waterman) and phylogenetic tree construction using Swiss-Prot data.


## Script and UI
* The code is designed to be used both as scripts with a purely textual UI, and also via web UI interfaces. Web UI is optional, but it is generally useful when browsing results or monitoring progress.
  * Wrappers over text UI can give give better more interactive progress indicators than the raw text scripts, for example tracking 'best so far' from a stream of results.
  * An HTML wrappers over search results can cross reference sequence IDs to sequence metadata and dynamically present alignments corresponding to a particular find.

## Python, C++ and Metal
* Most of the algorithms exist in a python version with sometimes C++ direct equivalents where speed is required.
  * LLMs can generate and debug python more rapidly than C++ and can easily call on the vast range of pre built python modules.
  * C++ versions, including arg parsing, can be direct translations of custom algorithms that need to run fast. Often these will take pre-prepared binary data as their database input, rather than read from text files.
  * Metal versions (which LLMs that don't have metal won't run/debug) exist for the long-running code that needs the greatest speed.

## 1. Core Applications

### Web Dashboard
**Entry Point:** `python py/web_server.py` (Run from project root)
**URL:** `http://localhost:8000` (Default)
**Status:** Mature
**Description:** A FastAPI-based web server wrapper that provides a dashboard for managing analysis jobs. It allows you to configure and run Data Munging (conversions of file formats for speed or making subsets), Computation (Tree Building from search results), and Sequence Search jobs (long running all-on-all sequence comparisons).

The UI allows multiple simultaneous jobs to be run, paused, resumed. Users can connect to the running jobs display from a remote machine and view the configuration parameters that were used for a job.

### Sequence Search (SW)
**Executable:** `bin/sw_search_metal` (Compiled from `c_src/sw_search_metal.mm`)
**Python Wrapper:** `py/sw_search.py`
**Status:** Mature (High Performance)
**Description:** A Metal-accelerated (GPU) implementation of the Smith-Waterman algorithm. It performs massive parallel searches of a query sequence against a database, producing protein similarity data - a list of high scoring links.
*   **Usage:** Typically invoked via the Web Dashboard. Can be run standalone via CLI.
*   **Data:** Requires binary compiled data (`fasta.bin`, `pam250.bin`) generated by `c_src/prepare_data.py`.

Typically this program is run iterating over all sequences in a database, to get an all on all comparison of proteins. The program can be interrupted, and when it resumes check its links file to see where to resume from.

### Tree Builder
**Script:** `py/tree_builder.py`
**Backend:** `bin/tree_builder_cpp` (Optional C++ acceleration)
**Status:** Active
**Description:** Constructs Minimum Spanning Trees (MST) from protein similarity data.
*   **Usage:** Configurable via `py/computation.py` jobs in the dashboard.

### Data Munger
**Script:** `py/data_munger.py`
**Status:** Active
**Description:** Filters and processes raw Swiss-Prot data to extract relevant proteins based on criteria (e.g., taxonomy, existence of GO terms).

## 2. Python Modules (`py/`)

*   **`job_manager.py`**: The orchestration engine. Manages the lifecycle (start, stop, status) of background jobs triggered by the web server.
*   **`sequences.py`**: The data access layer. Abstracts reading from Swiss-Prot (`.dat`) and FASTA files. Handles caching (Pickle) and Indexing for performance.
*   **`sw_align.py`**: A pure Python implementation of Smith-Waterman. Useful for debugging and verifying scores against the Metal implementation.
*   **`pam_converter.py`**: Utility to convert Biopython's PAM250 matrix into the specific 32x32 integer format required by the Metal kernel.
*   **`computation.py`**: Defines and manages `ComputationJob`s, primarily for Tree Building tasks.
*   **`taxa_lca.py`**: Utilities for determining the Lowest Common Ancestor (LCA) in taxonomy trees.
*   **`swiss_to_pdb.py`**: Tools for mapping Swiss-Prot entries to Protein Data Bank (PDB) structures.
*   **`kabsch_3d_align.py`**: Implementation of the Kabsch algorithm for aligning 3D protein structures.

## 3. Metal & C++ Source (`c_src/`, `metal/`)

*   **`c_src/sw_search_metal.mm`**: The production C++/Metal implementation of the search algorithm.
*   **`c_src/sw.metal`**: The Metal shader kernel code.
*   **`metal/sw_search_metal.py`**: **Prototype.** A Python implementation that mimics the Metal kernel logic. Useful for debugging the algorithm logic without a GPU.

## 4. Web Interface (`static/`)

Contains the frontend code (HTML/JS) for the dashboard.
*   `index.html`: Main layout.
*   `jobs.html`: Job list and creation.
*   `config_*.html`: Configuration forms for specific job types.
*   `app.js`: Main application logic, handling global polling and UI updates.
*   `job_monitor.js`: Shared JavaScript logic for configuration iframes and job status monitoring.
*   `monitor.js`: Specific monitoring logic.
*   `style.css`: Global stylesheets.
*   `monitor.css`: Styles specific to the job monitor interface.
*   `stream.js`: Handles streaming output from jobs.

## 5. System Architecture

1.  **User** interacts with **Web Dashboard**.
2.  **Web Server** creates a **Job** via **Job Manager**.
3.  **Job** runs a specific script or executable:
    *   *Search Job* -> Wraps `bin/sw_search_metal`.
    *   *Computation Job* -> Runs `py/tree_builder.py`.
    *   *Data Job* -> Runs `py/data_munger.py`.
4.  **Results** are monitored via the Dashboard.

## 6. Setup & Compilation

*   **Compile Native Code:** Run `./compile.sh` to build `bin/sw_search_metal` and `bin/tree_builder_cpp`.
*   **Python Path:** Ensure `PYTHONPATH` includes the `py/` directory (e.g., `PYTHONPATH=py python py/web_server.py`).
